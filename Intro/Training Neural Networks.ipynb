{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm going to train a Neural Network using backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by calculating the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3289, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0211, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network with a different kind of loss\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# With a log-softmax output, you want to use the negative log likelihood loss, nn.NLLLoss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "### Run this to check your work\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a module, autograd, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7799,  0.9862],\n",
      "        [-1.6316, -1.0634]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6082, 0.9727],\n",
      "        [2.6621, 1.1308]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x7fab7d093390>\n",
      "tensor(1.3434, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we can see the operation that created y\n",
    "y = x**2\n",
    "print(y)\n",
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)\n",
    "\n",
    "# same with z\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Autograd together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0024,  0.0024,  0.0024,  ...,  0.0024,  0.0024,  0.0024],\n",
      "        [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "        [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n",
      "        ...,\n",
      "        [ 0.0007,  0.0007,  0.0007,  ...,  0.0007,  0.0007,  0.0007],\n",
      "        [ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],\n",
      "        [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing needed to train the network is an optimizer that we'll use to update the weights with the gradients\n",
    "\n",
    "We get these from PyTorch's optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all these pieces together we can start training our network\n",
    "\n",
    "Here we're going to loop through trainloader to get our training batches.\n",
    "\n",
    "Note that I have a line of code optimizer.zero_grad(). When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8464087042600108\n",
      "Training loss: 0.8124597188887566\n",
      "Training loss: 0.5230150037546402\n",
      "Training loss: 0.4272247979254611\n",
      "Training loss: 0.3804115919447911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, then backward pass, then update weights\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADsCAYAAAAhDDIOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAV00lEQVR4nO3dfbRddX3n8feHBIIRCA6BDgQwaqOCMCIGCj5QLdrFgwOtgw4oWjsOVioOKGPL1I46dpxF7UjFJdZmAEVEUFAUeSgyVUAtIAkgzzhII0mQRyGAKOThO3+cE3p7vTu5XM7J3id5v9a6K+fs7z7nfs69N+d7f7/9u3unqpAkqWs2aTuAJEkTsUFJkjrJBiVJ6iQblCSpk2xQkqROskFJkjrJBiVpaJJ8LMmX284xFUm+mOR/TvGxa33dSW5J8rrx+ybZOcnjSaZNKfQGxgYl6VlJ8rYkC/tvrD9PckmS17SUpZL8sp9lWZKTuvhmX1Uvq6rLJ9h+d1VtUVWrAJJcnuQ/r/eAHWGDkjRlST4IfBr4X8BvATsDnwMObTHWy6tqC2B/4G3AUeN3SDJ9vafSM2aDkjQlSWYBHwfeV1XfqKpfVtWKqvp2VX2o4THnJrk3yfIkVyZ52ZjaQUluTfJYf/TzX/vbZye5MMkjSX6R5PtJ1vneVVW3A98Hdus/z+Ikf57kRuCXSaYn2aU/SnmkP+12yLinmZ3ksn6mK5I8f0zek5MsSfJokkVJXjvusZsn+Wr/sdclefmYxy5O8oYJvj5z+6PA6Uk+AbwW+Gx/RPjZJKck+dS4x3w7yXHr+nqMIhuUpKnaF9gcOP8ZPOYSYB6wHXAdcNaY2mnAn1TVlvSaynf7248HlgLb0hul/QWwznO0JdmV3hv89WM2HwEcDGwNBPg28J1+nvcDZyV5yZj93w78FTAbuGFc3muBPYB/A3wFODfJ5mPqhwLnjql/M8mm68q9RlV9mF6DPaY/7XcMcAZwxJoGnWQ2vZHi2ZN93lFig5I0VdsAD1bVysk+oKpOr6rHqupJ4GPAy/sjMYAVwK5Jtqqqh6vqujHbtwee3x+hfb/WfhLR65I8TK/5nAp8YUztM1W1pKp+BewDbAGcWFVPVdV3gQvpNbE1LqqqK/t5Pwzsm2Sn/mv5clU9VFUrq+pTwAxgbHNbVFXnVdUK4CR6zXyfyX6tJlJVPwKW02tKAIcDl1fVfc/mebvKBiVpqh6iNwU2qeM5SaYlOTHJT5M8Cizul2b3//0PwEHAz/rTafv2t/8NcCfwnSR3JTlhHZ9qz6p6XlW9qKr+sqpWj6ktGXN7B2DJuPrPgDkT7V9VjwO/6D+OJMcnua0/XfkIMGvMaxn/2NX0RoE7rCP7ZJwBHNm/fSRw5gCes5NsUJKm6irg18AfTHL/t9Gb9noDvTfzuf3tAaiqa6vqUHrTbd8Evtbf/lhVHV9VLwT+PfDBJPszNWNHXvcAO407nrUzsGzM/Z3W3EiyBb3punv6x5v+HHgr8Lyq2preyCYNj90E2LH/Oaead40vA4f2j2ntQu9rtUGyQUmakqpaDnwEOCXJHySZmWTTJAcm+eQED9kSeJLeyGsmvZV/ACTZLMnbk8zqT4k9CqxZav2mJL+dJGO2rxrAS7gG+CXwZ/3cr6PXAM8Zs89BSV6TZDN6x6Kuqaol/deyEngAmJ7kI8BW457/lUne3B9hHtd/7Vc/w4z3AS8cu6GqltI7/nUm8PX+dOUGyQYlacqq6iTgg8Bf0nuzXgIcw8S/1X+J3hTaMuBWfvPN+h3A4v7033v5l2msecD/BR6nN2r73ER/QzSF7E8BhwAHAg/SWx7/zv7qvzW+AnyU3tTeK+ktmgC4lN6Cj5/0X9Ov+dfThwDfAv4j8HD/tb2533yfiZOBw5I8nOQzY7afAezOBjy9BxAvWChJoyXJfvSm+uaOO4a2QXEEJUkjpL9U/Vjg1A25OYENSpJGRpJdgEfoLbv/dMtxhs4pPklSJ6317xfeuMlb7F7a6F22+tysey9Jg+YUnySpkzyjr9Si2bNn19y5c9uOIbVq0aJFD1bVtuO326CkFs2dO5eFCxe2HUNqVZKfTbTdKT5JUifZoCRJnWSDkiR1kg1KktRJNihJUifZoCRJneQyc6lFNy1bztwTLmo7hoZk8YkHtx1hpDmCkiR1kg1KktRJNihJUifZoKQBS3JskpuT3JLkuLbzSKPKBiUNUJLdgKOAvYGXA29KMq/dVNJoskFJg7ULcHVVPVFVK4ErgD9sOZM0kmxQ0mDdDOyXZJskM4GDgJ3G7pDkPUkWJlm46onlrYSURoF/ByUNUFXdluSvgcuAx4EfAyvH7bMAWAAwY/t5XrVaauAIShqwqjqtqvasqv2AXwD/r+1M0ihyBCUNWJLtqur+JDsDbwb2bTuTNIpsUNLgfT3JNsAK4H1V9XDbgaRRZIOSBqyqXtt2BmlD4DEoSVInOYKSWrT7nFks9IzX0oQcQUmSOskGJUnqJBuUJKmTbFBSi25a5qmOpCY2KElSJ9mgJEmdZIOSBizJB/oXK7w5ydlJNm87kzSKbFDSACWZA/wXYH5V7QZMAw5vN5U0mmxQ0uBNB56TZDowE7in5TzSSLJBSQNUVcuA/w3cDfwcWF5V32k3lTSabFDSACV5HnAo8AJgB+C5SY4ct49X1JUmwQYlDdYbgH+uqgeqagXwDeBVY3eoqgVVNb+q5k+bOauVkNIosEFJg3U3sE+SmUkC7A/c1nImaSTZoKQBqqprgPOA64Cb6P0fW9BqKGlEebkNacCq6qPAR9vOIY06R1CSpE6yQUmSOskGJbVo9zmu4pOa2KAkSZ1kg5IkdZKr+KQW3bRsOXNPuKjtGBNafOLBbUfQRs4RlCSpkxxBDdBTB+zVWFvyzhWNtaN2/2Fj7cAtb2qs3bVidmPt+AuPbKzNeKj595Ifvfekxtphf3hUY62ubc4pSVPhCEqS1Ek2KGmAkrwkyQ1jPh5NclzbuaRR5BSfNEBVdQewB0CSacAy4PxWQ0kjyhGUNDz7Az+tqp+1HUQaRTYoaXgOB84ev9ELFkqTY4OShiDJZsAhwLnja16wUJocj0E9Q4++bZ/G2jEf/Y33oqfd9qsdGmtnnvXGxtoVp81trK164IHG2kufv6yx9skrvtZYe2z1qsbaJsufaM7SWNloHQhcV1X3tR1EGlWOoKThOIIJpvckTZ4NShqwJDOBNwLfaDuLNMqc4pMGrKqeALZpO4c06hxBSZI6yRGU1KLd58xioWcNlybkCEqS1EmOoCbw8B/t21i77BPNZ/t+5RV/2lh70TtubKzNWf1PjbW1Lt9OGku3nrB9Y+2lm85orO216IjG2rY/uWNtaSRpoBxBSZI6yQYlSeokG5QkqZNsUJKkTrJBSQOWZOsk5yW5PcltSZpX3Uhq5Co+afBOBv6hqg7rn9V8ZtuBpFFkg5rAy46+ubH23+/dr7H24mMWN9ZWreUs4VO1er89Gmt3HvL5KT3nr5/adKpxBCTZCtgPeBdAVT0FPNVmJmlUOcUnDdYLgQeALyS5PsmpSZ7bdihpFNmgpMGaDuwJ/F1VvQL4JXDC2B3GXlH3gbVc00va2NmgpMFaCiytqmv698+j17CeNvaKuttuu+16DyiNChuUNEBVdS+wJMlL+pv2B25tMZI0slwkIQ3e+4Gz+iv47gL+uOU80kiyQUkDVlU3APPbziGNuo22Qa3+3Vc01k7Z8e8ba3udclxjbceHm89KPgxzP/mTxtpFT2zRWNtzxv2NtV89svmzyiRJg+IxKElSJ9mgJEmdZIOSJHWSDUqS1Ekb7SIJqQtuWracuSdc9PT9xSce3GIaqVscQUmSOmmjHUGteG7zS3+yVjbWdj75hsba6meVaGKPv3WfxtondvhUY+13vv2Bxtqdhzaf6fz552dywSRpyBxBSZI6aaMdQUnDkmQx8BiwClhZVZ5VQpoCG5Q0HK+vqgfbDiGNMqf4JEmdZIOSBq+A7yRZlOQ944tjL1i46onlLcSTRoNTfNLgvbqq7kmyHXBZktur6so1xapaACwAmLH9vGorpNR1G22Dmv6rVY21mZts2li7/aTdGmsvPe7Gxtom/3a7xtoje+/QWPvUiac01n7nkuYzq2/yZPPgeG1nOt/8Oz9urPlOOjlVdU//3/uTnA/sDVy59kdJGs8pPmmAkjw3yZZrbgO/D9zcbippNG20IyhpSH4LOD8J9P5/faWq/qHdSNJoskFJA1RVdwEvbzuHtCFwik+S1EmOoKQW7T5nFgs9g7k0IUdQkqRO2mhHUNO+d11j7RWfP7axdud7P9tYO+f12zbW9tr8HxtrL5r+nMbav7v6nY21lx5/e2Ptdf90b2PtS/e+qrFWKzw7j6RucAQlSeqkjXYEJXXB+CvqrotX3NXGxBGUJKmTbFCSpE6yQUmSOskGJQ1BkmlJrk9yYdtZpFHlIokJ7PzJRY21/Rf9SWPtsaObr+2zYtW0xtqMb23dWNvpzGsba6tXNZ+RfdmTzc+5+1b3NNaumj6zsVYrVzbW9BuOBW4Dtmo7iDSqHEFJA5ZkR+Bg4NS2s0ijzAYlDd6ngT8DVk9U9Iq60uTYoKQBSvIm4P6qapwnrqoFVTW/quZPmzlrPaaTRosNShqsVwOHJFkMnAP8XpIvtxtJGk02KGmAquq/VdWOVTUXOBz4blUd2XIsaSTZoCRJneQy8wnUk0821mZc3Lzse8bFQ8iyltomM5uXhL9qy9saax/5+uGNtResvGoysTQJVXU5cHnLMaSR5QhKktRJjqCkFnlFXamZIyhJUifZoCRJnWSDklr0TC9YKG1MbFCSpE5ykcQIW/q+PRprb9niB421M855uLE24cnjJKkFjqAkSZ1kg5IGKMnmSX6U5MdJbknyP9rOJI0qp/ikwXoS+L2qejzJpsAPklxSVVe3HUwaNTYoaYCqqoDH+3c37X+s7YxVkho4xScNWJJpSW4A7gcuq6pr2s4kjSIblDRgVbWqqvYAdgT2TrLb2LpX1JUmxym+EbbZfg821i771XMaa1ly3zDiaJyqeiTJ5cABwM1jti8AFgDM2H6e039SA0dQ0gAl2TbJ1v3bzwHeANzebippNDmCkgZre+CMJNPo/QL4taq6sOVM0kiyQUkDVFU3Aq9oO4e0IXCKT5LUSTYoSVIn2aCkFu0+ZxaLvaKuNCGPQY2wj+9yQWPt6Mvf0Vh78cMLhxFHkgbKEZQkqZNsUFKLblrmmSSkJjYoSVIn2aAkSZ1kg5IkdZINShqgJDsl+V6S2/pX1D227UzSqHKZecfd86FXNdZes/lVjbV5X1g5jDhat5XA8VV1XZItgUVJLquqW9sOJo0aR1DSAFXVz6vquv7tx4DbgDntppJGkw1KGpIkc+mdOPaacdu9YKE0CTYoaQiSbAF8HTiuqh4dW6uqBVU1v6rmT5s5q52A0giwQUkDlmRTes3prKr6Rtt5pFFlg5IGKEmA04DbquqktvNIo8xVfB2Q6c3fhle95frG2r5XH9VY2+mHNzyrTJqyVwPvAG5Ksuab8BdVdXGLmaSRZIOSBqiqfgCk7RzShsApPklSJ9mgpBbtPsdVfFITG5QkqZNsUJKkTrJBSZI6yVV8HfDI4fMbaxfN+Vxjba8v7DGMOFqPvKKu1MwRlCSpk2xQkqROskFJA5Tk9CT3J7m57SzSqLNBSYP1ReCAtkNIGwIblDRAVXUl8Iu2c0gbAhuUJKmTXGbeAfe9enVj7aanVjTWtj//rsbaymeVSMOU5D3AewCmbbVty2mk7nIEJa1nXlFXmhwblCSpk2xQ0gAlORu4CnhJkqVJ3t12JmlUeQxKGqCqOqLtDNKGwhGUJKmTbFCSpE5yim89mbZt83Li8w74bGPt7df9p8bajvfe8qwyqX1eUVdq5ghKktRJNihJUifZoKQWecFCqZkNSpLUSTYoSVIn2aAkSZ3kMvP15N7Dfrux9soZmzXWtvjWVsOIoyFKcgBwMjANOLWqTmw5kjSSHEFJA5RkGnAKcCCwK3BEkl3bTSWNJhuUNFh7A3dW1V1V9RRwDnBoy5mkkWSDkgZrDrBkzP2l/W1PS/KeJAuTLFz1hMvMpSY2KGmwMsG2+ld3vGChNCk2KGmwlgI7jbm/I3BPS1mkkWaDkgbrWmBekhck2Qw4HLig5UzSSHKZ+XryyMtWN9b++qF5jbVtLv1pY23Vs0qkYaiqlUmOAS6lt8z89KrytPPSFNigpAGrqouBi9vOIY06p/gkSZ1kg5Ja5AULpWY2KElSJ9mgJEmdZIOSJHWSq/jWk3nHXNNY+y7PXcsj7x98GEkaAY6gJEmdZIOSJHWSDUqS1Ek2KElSJ7lIQmrRokWLHk9yR9s5xpgNPNh2iD6zTGxDzPL8iTbaoKR23VFV89sOsUaShV3JY5aJbUxZ1tqgLlt97kQXX5Mkaeg8BiVJ6iQblNSuBW0HGKdLecwysY0mS6pqmM8vSdKUOIKSJHWSDUpaD5IckOSOJHcmOWGC+owkX+3Xr0kyt8UsH0xya5Ibk/xjkgmXAK+PLGP2OyxJJRnq6rXJ5Eny1v7X55YkX2krS5Kdk3wvyfX979VBQ8pxepL7k9zcUE+Sz/Rz3phkz4F98qryww8/hvgBTAN+CrwQ2Az4MbDruH3+FPh8//bhwFdbzPJ6YGb/9tFtZunvtyVwJXA1ML/l79M84Hrgef3727WYZQFwdP/2rsDiIWXZD9gTuLmhfhBwCRBgH+CaQX1uR1DS8O0N3FlVd1XVU8A5wKHj9jkUOKN/+zxg/yTD+DOPdWapqu9V1RP9u1cDOw4hx6Sy9P0V8Eng10PK8UzyHAWcUlUPA1TVsC43MJksBWzVvz0LuGcYQarqSuAXa9nlUOBL1XM1sHWS7QfxuW1Q0vDNAZaMub+0v23CfapqJbAc2KalLGO9m95vx8OwzixJXgHsVFUXDinDM8oDvBh4cZIfJrk6yQEtZvkYcGSSpcDFwPuHlGVdnunP1KR5Jglp+CYaCY1fPjuZfdZXlt6OyZHAfOB3h5BjnVmSbAL8LfCuIX3+Z5Snbzq9ab7X0RtZfj/JblX1SAtZjgC+WFWfSrIvcGY/y+oBZ1mXof3sOoKShm8psNOY+zvym9MxT++TZDq9KZu1TasMMwtJ3gB8GDikqp4cQo7JZNkS2A24PMliesc3LhjiQonJfp++VVUrquqfgTvoNaw2srwb+BpAVV0FbE7v3Hjr26R+pqbCBiUN37XAvCQvSLIZvUUQF4zb5wLgj/q3DwO+W/0j0Os7S39a7e/pNadhXtJ5rVmqanlVza6quVU1l97xsEOqamEbefq+SW8RCUlm05vyu6ulLHcD+/ez7EKvQT0whCzrcgHwzv5qvn2A5VX180E8sVN80pBV1cokxwCX0luddXpV3ZLk48DCqroAOI3eFM2d9EZOh7eY5W+ALYBz++s07q6qQ1rKst5MMs+lwO8nuRVYBXyoqh5qKcvxwP9J8gF6U2rvGsYvNUnOpjelObt/vOujwKb9nJ+nd/zrIOBO4Angjwf2uYfzS5okSc+OU3ySpE6yQUmSOskGJUnqJBuUJKmTbFCSpE6yQUmSOskGJUnqJBuUJKmT/j+9ub57f6h77wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking out the predictions with our network trained\n",
    "\n",
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
